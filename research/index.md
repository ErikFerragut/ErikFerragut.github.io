---
title: Research Interests
layout: default
headline: Research interests in data science
---
This page describes research I have done at ORNL since 2009. Many topics are ongoing, depending on availability of funding. Results have been communicated with the sponsor and, in some cases, published in the scientific literature.

## Deep Learning ##

### Sequence Classification using Deep Learning ###

Catching domains that were generated by an algorithm.

### Multiscale Architectures for Deep Learning ###

Language Modeling; Sequence Classification
Multiscale analysis. LSTM.

### Manifold Learning ###

### Advanced Language Models ###

### Learning from Simulation Data ###

## Cyber-Physical Attack Detection (CPAD) ##

### Power Grid Analysis ###

In a physical system, such as a power grid, there are often many sensors with views into the same physical state. Inevitably, these sensors must then have some dependencies. In the case of power grid measurements, current and voltage are related by Kirchhoff's Laws. In this research we attempt to learn those relationships using machine learning (neural networks) and then use that model to detect data that has been tampered with, either because of component failure or because of malicious intent. A demonstration of the detection was developed. The trained algorithm was able to improve detection of replay attacks by a factor of 30 or more on both simulated and real data.

[POWER POINT SLIDES: *Erik M. Ferragut*, Jason Laska, "Cyber Physical Attack Detection," Presentation, May 2017.](http://erikferragut.me/files/ttp-cpad-4-2017May.pptx)

### Validation Across Thousands of Simulated Systems ###

Work is ongoing to apply CPAD to a range of thousands of simulated control systems. This analysis will allow us to (1) validate the learned physical laws, and (2) identify properties of systems that make CPAD especially well suited or poorly suited.

### Applications to Industrial Plants, Vehicles, or Other Systems ###

CPAD should apply to any system with multiple views into the same physical space. We are interested in pursuing pilots to explore these applications on real, operationally relevant data.



## Data Wrangling ##

### Field Matching ###

Created and deployed novel field-matching algorithm to merge
medical provider data. Publication.

### Data Pre-Processing ###

ORDEAL
## Anomaly Detection ##

### Defining Anomaly Detection ###

Anomaly detection is the attempt to find outliers. On the one hand, you can think of it as finding points that are far from the concentrations of probability. On the other hand, you can think of it as finding points that have low probability density. I have adopted and argued for the second approach. In particular, given a probability measure <span>$p$</span>, we define a tail probability <span>$T(x)$</span> of a point <span>$x$</span> to be:

<div>
\[
    T(x) = p( \{ y \mid dp(y) \leq dp(x) \} ),
\]
</div>

the probability of the measure <span>$p$</span> generating an event that is no more likely than <span>$x$</span> (where <span>$dp(x)$</span> is the derivative of <span>$p$</span> with respect to some background measure, in the Radon-Nikodym sense).  This is not the only way to define a tail probability. To be more specific, we can call it *meta-rarity* because it is a number indicating the rarity of the rarity. This definition has a number of advantages, such as allowing for the regulation of alerts that are generated according to <span>$p$</span>. Another advantage is that since the tail probabilities is a sort of p-value (as in statistical significance), tail probabilities derived from different distributions are directly comparable, even if the underlying data are vastly different.

[PAPER: *EM Ferragut*, J Laska, RA Bridges, "A new, principled approach to anomaly detection," International Conference on Machine Learning and Applications (ICMLA), 2012.](http://erikferragut.me/files/ferragut2012new.pdf)

### Meta-Rarity Is Optimal ###

The definition of tail probabilities given above has another advantage. If it is the case that your data are being generated according to </span>$p$</span>, then it is the best way to distinguish it from data that is generated uniformly over the same space. The mathematical details of this statement require a lot of explanation, which is being written up in a paper. One way to think of it is as a variation of the Neyman-Pearson Lemma where one of the probability distributions is replaced with a (not necessarily finite) measure. (Paper in progress.)

### Scalable Models ###

To make this work deployable and useful, a way of building and updating models from streaming data was required. To do this, we focused on discretization of data (i.e.,
binned data). In this application, the discrete distribution is being estimated from the event counts, and the events are being scored for anomalousness with respect to the estimate of the distribution. The tail probability definition applies in this case since we are looking at a discrete measure. There are two main questions to address to make this scale:

1. _How do we deal with previously unseen events?_ This is the question of priors and smoothing. The typical solution, Cauchy Smoothing, is to add a "virtual" observation (or a fraction of an observation) to each value before computing probabilities. However, this method breaks down if the space of possible values is not known in advance. In that case, a typical prior to use would be a [dirichlet process prior](http://https://en.wikipedia.org/wiki/Dirichlet_process) (aka stick breaking). However, this approach is most appropriate when the prior is a collection of countably many point masses drawn from a continuous probability distribution. It could probably be made to work, but it would require assuming a "background" distribution on all possible values, which kind of brings us back to the original issue. Instead, we adopted a "hack" which is simple and mathematically unjustified, but solves the problem. In particular, we simply augment the count of the new observation first and then compute its tail probability. The result is a robust anomaly score that tempers the anomalousness of a never before seen event with some notion of how much overall data has been seen.

2. _How do we efficiently compute the tail probability?_ The simplest approach to computing the tail probability is to first sort the counts, and then sum up over all values not bigger than the one observed. However, then with each new observation, it may become necessary to move a value through the list to its new position in order to resort the list. By grouping events that have been observed the same number of times, this can be made much faster. Additional speed-ups were also explored and implemented (by other researchers).

This has been implemented into the ORNL system called Situ, and has been deployed at multiple sites.

[PATENT: *Erik M. Ferragut*, John R. Goodall, Michael D. Iannacone, Jason A. Laska, Lane T. Harrison, "Real-time detection and classification of anomalous events in streaming data," US Patent US9319421 B2, Apr 19, 2016.](https://www.google.com/patents/US9319421)

[SITE: ORNL, "Project: Situ", October 2016](https://www.ornl.gov/division/projects/situ)

### Importance Sampling ###

For discrete distributions, computing tail probabilities is a matter of counting events that are as rare or more rare. For continuous distributions, we lose the computational benefits of discretization. In particular, no two events can be expected to have the same probability. For most realistic (i.e., multivariate) distributions, even simple ones, the tail probabilities are hard to compute. The main exceptions are the multivariate normal distribution, whose tail probabilities are described using a Chi-squared distribution, and a vector of independent and identically distribution exponential variables, whose tail probabilities are described using a Gamma distribution. (We have yet to find a reference for this latter result, but it was likely already known.)

In more general distributions, computing tail probabilities has no closed form solution. Operationally, this presents a huge difficulty. Given an observation, how can we estimate its tail probability. By looking at methods that combine Monte Carlo and importance sampling with regressions, we found we were able to improve the computational cost of estimating tail probabilities by many orders of magnitude. (Paper in progress.)


### Change Point Detection ###

One interesting observation about tail probabilities is that they are approximately uniformly distributed, differing from uniform only when there are sets of positive measure that have constant probability density (i.e., level sets of positive probability). This observation enables a method to measure the extent to which the underlying distribution <span>$p$</span> actually matches how the data are generated. If the tail probabilities are uniformly distributed, then the assumption is supported. If the probabilities deviate from a uniform distribution, then the assumption must be rejected. We intend to integrate this into our anomaly detection system, Situ. (Paper in progress.)


## Game Theory ##


### Cyber Game Strategy Analysis ###

Cyber security lends itself to game-theoretic analysis because it is, foundationally, a contest between attacker and defender, or else between two agressors. Previous work that attempted to use game-theoretic analysis of cyber conflicts were overly simplistic so as to lend themselves to better analysis. One common simplification is that it is an iterated game where the players move simultaneously. In our work, we developed a game that is interesting because it is (1) potentially multiplayer, (2) based on turn-taking, (3) has incomplete information, and (4) involves probabilistic outcomes. These properties allow the game to be of much greater fidelity to real life conflicts. We then modeled each players knowledge of the game using Bayesian inference, and considered different strategies for making decisions based on the given information. The results confirmed some well known rules of thumb in cyber security, such as the importance of patching systems. We also discovered that with great uncertainty, the best strategy basically ignores what the opponents are doing. (This paper won the best paper award at its venue. Also of note, my co-authors were middle school and high school students.)

[PAPER: *Erik M Ferragut*, Andrew C Brady, Ethan J Brady, Jacob M Ferragut, Nathan M Ferragut, Max C Wildgruber, "HackAttack: Game-Theoretic Analysis of Realistic Cyber Conflicts," Proceedings of the 11th Annual Cyber and Information Security Research Conference, April 2016.](http://erikferragut.me/files/ferragut2015hackattack.pdf)

### Strategies from Reinforcement Learning ###

The greatest difficulty in the cyber game strategy analysis was that the combination of uncertainties was enormous, making the Bayesian inference burdensome. The next step in this work is to drop the explicit tracking of probabilities and develop a strategy optimization process based on reinforcement learning. (Ongoing)



## Quantum Computation ##

### Monte Carlo Simulation ###
Simulations and Clifford Groups. Publication.

### Decomposition of Quantum Processes into Mixtures of Cliffords ###

### Special Case Simulations ###

## Compressed Sensing ##

Idea; never published!

## Matching Term Trees ##

Sudoku trick.






